{"@context":{"@language":"en","@vocab":"https://schema.org/","arrayShape":"cr:arrayShape","citeAs":"cr:citeAs","column":"cr:column","conformsTo":"dct:conformsTo","cr":"http://mlcommons.org/croissant/","data":{"@id":"cr:data","@type":"@json"},"dataBiases":"cr:dataBiases","dataCollection":"cr:dataCollection","dataType":{"@id":"cr:dataType","@type":"@vocab"},"dct":"http://purl.org/dc/terms/","extract":"cr:extract","field":"cr:field","fileProperty":"cr:fileProperty","fileObject":"cr:fileObject","fileSet":"cr:fileSet","format":"cr:format","includes":"cr:includes","isArray":"cr:isArray","isLiveDataset":"cr:isLiveDataset","jsonPath":"cr:jsonPath","key":"cr:key","md5":"cr:md5","parentField":"cr:parentField","path":"cr:path","personalSensitiveInformation":"cr:personalSensitiveInformation","recordSet":"cr:recordSet","references":"cr:references","regex":"cr:regex","repeated":"cr:repeated","replace":"cr:replace","sc":"https://schema.org/","separator":"cr:separator","source":"cr:source","subField":"cr:subField","transform":"cr:transform"},"@type":"sc:Dataset","distribution":[{"@type":"cr:FileObject","@id":"repo","name":"repo","description":"The Hugging Face git repository.","contentUrl":"https://huggingface.co/datasets/JingweiNi/LEXam/tree/refs%2Fconvert%2Fparquet","encodingFormat":"git+https","sha256":"https://github.com/mlcommons/croissant/issues/80"},{"@type":"cr:FileSet","@id":"parquet-files-for-config-mcq_4_choices","containedIn":{"@id":"repo"},"encodingFormat":"application/x-parquet","includes":"mcq_4_choices/*/*.parquet"},{"@type":"cr:FileSet","@id":"parquet-files-for-config-mcq_perturbation","containedIn":{"@id":"repo"},"encodingFormat":"application/x-parquet","includes":"mcq_perturbation/*/*.parquet"},{"@type":"cr:FileSet","@id":"parquet-files-for-config-open_question","containedIn":{"@id":"repo"},"encodingFormat":"application/x-parquet","includes":"open_question/*/*.parquet"}],"recordSet":[{"@type":"cr:RecordSet","dataType":"cr:Split","key":{"@id":"mcq_4_choices_splits/split_name"},"@id":"mcq_4_choices_splits","name":"mcq_4_choices_splits","description":"Splits for the mcq_4_choices config.","field":[{"@type":"cr:Field","@id":"mcq_4_choices_splits/split_name","dataType":"sc:Text"}],"data":[{"mcq_4_choices_splits/split_name":"test"}]},{"@type":"cr:RecordSet","@id":"mcq_4_choices","description":"JingweiNi/LEXam - 'mcq_4_choices' subset","field":[{"@type":"cr:Field","@id":"mcq_4_choices/split","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"fileProperty":"fullpath"},"transform":{"regex":"mcq_4_choices/(?:partial-)?(test)/.+parquet$"}},"references":{"field":{"@id":"mcq_4_choices_splits/split_name"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Question","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Question"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Choices","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Choices"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Gold","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Gold"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Course","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Course"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Language","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Language"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Area","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Area"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Jurisdiction","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Jurisdiction"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Year","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Year"}}},{"@type":"cr:Field","@id":"mcq_4_choices/n_statements","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"n_statements"}}},{"@type":"cr:Field","@id":"mcq_4_choices/none_as_an_option","dataType":"sc:Boolean","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"none_as_an_option"}}},{"@type":"cr:Field","@id":"mcq_4_choices/Id","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_4_choices"},"extract":{"column":"Id"}}}]},{"@type":"cr:RecordSet","dataType":"cr:Split","key":{"@id":"mcq_perturbation_splits/split_name"},"@id":"mcq_perturbation_splits","name":"mcq_perturbation_splits","description":"Splits for the mcq_perturbation config.","field":[{"@type":"cr:Field","@id":"mcq_perturbation_splits/split_name","dataType":"sc:Text"}],"data":[{"mcq_perturbation_splits/split_name":"test"}]},{"@type":"cr:RecordSet","@id":"mcq_perturbation","description":"JingweiNi/LEXam - 'mcq_perturbation' subset","field":[{"@type":"cr:Field","@id":"mcq_perturbation/split","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"fileProperty":"fullpath"},"transform":{"regex":"mcq_perturbation/(?:partial-)?(test)/.+parquet$"}},"references":{"field":{"@id":"mcq_perturbation_splits/split_name"}}},{"@type":"cr:Field","@id":"mcq_perturbation/question","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"question"}}},{"@type":"cr:Field","@id":"mcq_perturbation/4_choices","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"4_choices"}}},{"@type":"cr:Field","@id":"mcq_perturbation/4_choices_answer","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"4_choices_answer"}}},{"@type":"cr:Field","@id":"mcq_perturbation/8_choices","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"8_choices"}}},{"@type":"cr:Field","@id":"mcq_perturbation/8_choices_answer","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"8_choices_answer"}}},{"@type":"cr:Field","@id":"mcq_perturbation/16_choices","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"16_choices"}}},{"@type":"cr:Field","@id":"mcq_perturbation/16_choices_answer","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"16_choices_answer"}}},{"@type":"cr:Field","@id":"mcq_perturbation/32_choices","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"32_choices"}}},{"@type":"cr:Field","@id":"mcq_perturbation/32_choices_answer","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"32_choices_answer"}}},{"@type":"cr:Field","@id":"mcq_perturbation/course","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"course"}}},{"@type":"cr:Field","@id":"mcq_perturbation/language","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"language"}}},{"@type":"cr:Field","@id":"mcq_perturbation/n_statements","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"n_statements"}}},{"@type":"cr:Field","@id":"mcq_perturbation/id","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-mcq_perturbation"},"extract":{"column":"id"}}}]},{"@type":"cr:RecordSet","dataType":"cr:Split","key":{"@id":"open_question_splits/split_name"},"@id":"open_question_splits","name":"open_question_splits","description":"Splits for the open_question config.","field":[{"@type":"cr:Field","@id":"open_question_splits/split_name","dataType":"sc:Text"}],"data":[{"open_question_splits/split_name":"test"},{"open_question_splits/split_name":"dev"}]},{"@type":"cr:RecordSet","@id":"open_question","description":"JingweiNi/LEXam - 'open_question' subset\n\nAdditional information:\n- 2 splits: test, dev","field":[{"@type":"cr:Field","@id":"open_question/split","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"fileProperty":"fullpath"},"transform":{"regex":"open_question/(?:partial-)?(test|dev)/.+parquet$"}},"references":{"field":{"@id":"open_question_splits/split_name"}}},{"@type":"cr:Field","@id":"open_question/Question","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Question"}}},{"@type":"cr:Field","@id":"open_question/Answer","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Answer"}}},{"@type":"cr:Field","@id":"open_question/Course","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Course"}}},{"@type":"cr:Field","@id":"open_question/Language","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Language"}}},{"@type":"cr:Field","@id":"open_question/Area","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Area"}}},{"@type":"cr:Field","@id":"open_question/Jurisdiction","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Jurisdiction"}}},{"@type":"cr:Field","@id":"open_question/Year","dataType":"cr:Int64","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"Year"}}},{"@type":"cr:Field","@id":"open_question/ID","dataType":"sc:Text","source":{"fileSet":{"@id":"parquet-files-for-config-open_question"},"extract":{"column":"ID"}}}]}],"conformsTo":"http://mlcommons.org/croissant/1.1","name":"LEXam","description":"\n  \n  \n    LEXam: Benchmarking Legal Reasoning on 340 Law Exams\n    A diverse, rigorous evaluation suite for legal AI from Swiss, EU, and international law examinations.\n  \n\n\n\n\t\n\t\t\n\t\n\t\n\t\t[GitHub Repo]\n\t\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ”¥ News\n\t\n\n\n[2025/05] Release of the first version of paper, where we evaluate 20+ representative SoTA LLMs with evaluations stricly verified by legal experts.\n\n\n\t\n\t\t\n\t\n\t\n\t\tðŸ§© Subsets\n\t\n\n\nmcq_4_choices: The standard 1660 MCQs of LEXam with 4 choices.\nmcq_perturbation: We find thatâ€¦ See the full description on the dataset page: https://huggingface.co/datasets/JingweiNi/LEXam.","alternateName":["JingweiNi/LEXam"],"creator":{"@type":"Person","name":"Jingwei Ni","url":"https://huggingface.co/JingweiNi"},"keywords":["cc-by-4.0","1K - 10K","parquet","Tabular","Text","Datasets","pandas","Croissant","Polars","ðŸ‡ºðŸ‡¸ Region: US"],"license":"https://choosealicense.com/licenses/cc-by-4.0/","url":"https://huggingface.co/datasets/JingweiNi/LEXam"}